{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a053283d-2336-48ec-8ee1-38ec5eef3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from loguru import logger\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    Qwen2Config,\n",
    "    Qwen2ForCausalLM,\n",
    "    Qwen2Tokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4b4ac-3940-4270-b40c-11f8a410cea5",
   "metadata": {},
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6a0c7-f421-49b3-84c1-2ce04fe277ec",
   "metadata": {},
   "source": [
    "# Суть проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad5c3f-f96d-4757-b617-7394d248e6aa",
   "metadata": {},
   "source": [
    "Можно описать её одним предложением: выяснить кто в интернете не прав. \n",
    "\n",
    "https://jeremybernste.in/writing/deriving-muon  - существует блог товарища, который писал оптимизатор MuON, в этом посте он описывает что же конкретно с точки зрения математики происходит в оптимизаторе. К нему в комменты приходит другой человек и задаёт резонный вопрос: \n",
    "\n",
    "\"Вы говорите, что у вас совсем не эврестический подход к построению вашего оптимизатор, чем вы гордитесь, но выбор приведения нормы RMS->RMS кажется ни на чем не основан, может есть другие варианты\"? \n",
    "\n",
    "И уже дальше второй комментатор выдывигает тезис: \"конкретно для задачи классификации кажется более логичным было бы использовать L1->RMS\". \n",
    "\n",
    "Задача: попытаться выяснить так ли это.\n",
    "\n",
    "Для данного таска была выбрана задача классификации, коль по условию мы исходим из последнего комментария, при этом наш датасет должен занимать время на обработку, не быть совсем легковесным и в то же время его должно быть возможно прогнать на моей локальной машине с 6gb VRAM и ~12 гб ОЗУ, так как все бесплатные коллабы к моменту написания этого введения я уже съел.\n",
    "\n",
    "Для этого возьму данные и задачу по классификации комментариев на токсичные и нормальные \n",
    "\n",
    "- Данные: https://habr.com/ru/companies/ru_mts/articles/585804/\n",
    "- Нейросеть нагенерю сам какую-то супер простую\n",
    "\n",
    "На основе одних и тех же данных и единой структуры нейросети попробую посмотреть на сходимость и скорость тренировки MuON, MuON_L1_RMS и ADAMW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e6639-d91a-47ec-ad91-ced0ec6a4905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>Вонючий совковый скот прибежал и ноет. А вот и...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>А кого любить? Гоблина тупорылого что-ли? Или ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14409</th>\n",
       "      <td>Посмотрел Утомленных солнцем 2. И оказалось, ч...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14411</th>\n",
       "      <td>До сих пор пересматриваю его видео. Орамбо кст...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14412 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic\n",
       "0                   Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1      Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                              Собаке - собачья смерть\\n    1.0\n",
       "3      Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4      тебя не убедил 6-страничный пдф в том, что Скр...    1.0\n",
       "...                                                  ...    ...\n",
       "14407  Вонючий совковый скот прибежал и ноет. А вот и...    1.0\n",
       "14408  А кого любить? Гоблина тупорылого что-ли? Или ...    1.0\n",
       "14409  Посмотрел Утомленных солнцем 2. И оказалось, ч...    0.0\n",
       "14410  КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...    1.0\n",
       "14411  До сих пор пересматриваю его видео. Орамбо кст...    0.0\n",
       "\n",
       "[14412 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_another = pd.read_csv('data/toxic/labeled.csv', on_bad_lines='skip', sep=',', # оба датафрейма читались из хабра МТС выше.\n",
    "                    encoding='UTF-8',\n",
    "                    engine='python',)\n",
    "data_another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa518be5-3bf3-4227-a8f5-9844b45a1ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__INSULT скотина! что сказать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__NORMAL я сегодня проезжала по рабочей...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__NORMAL очередной лохотрон. зачем прид...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__NORMAL ретро дежавю ... сложно понять...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__NORMAL а когда мы статус агрогородка ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248285</th>\n",
       "      <td>__label__NORMAL правильно всё по пять (5)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248286</th>\n",
       "      <td>__label__INSULT ёбанные нубы заходите на серве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248287</th>\n",
       "      <td>__label__NORMAL а у меня наверное рекорд в 196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248288</th>\n",
       "      <td>__label__NORMAL спасибо всем большое)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248289</th>\n",
       "      <td>__label__NORMAL нельзя ли увеличить хотя бы в ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248290 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0                    __label__INSULT скотина! что сказать\n",
       "1       __label__NORMAL я сегодня проезжала по рабочей...\n",
       "2       __label__NORMAL очередной лохотрон. зачем прид...\n",
       "3       __label__NORMAL ретро дежавю ... сложно понять...\n",
       "4       __label__NORMAL а когда мы статус агрогородка ...\n",
       "...                                                   ...\n",
       "248285       __label__NORMAL правильно всё по пять (5)...\n",
       "248286  __label__INSULT ёбанные нубы заходите на серве...\n",
       "248287  __label__NORMAL а у меня наверное рекорд в 196...\n",
       "248288              __label__NORMAL спасибо всем большое)\n",
       "248289  __label__NORMAL нельзя ли увеличить хотя бы в ...\n",
       "\n",
       "[248290 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_another_one = pd.read_table('data/toxic/dataset.txt', on_bad_lines='skip',  # оба датафрейма читались из хабра МТС выше.\n",
    "                    encoding='UTF-8',\n",
    "                    engine='python', header=None)\n",
    "and_another_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1621498-e09e-4dea-be60-9846739417a9",
   "metadata": {},
   "source": [
    "Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa7beb9-6298-44f2-85c7-29b4c0e6ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_toxicity(x):\n",
    "    if x[0:15] == '__label__NORMAL':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17118f64-7053-4280-bc8b-3c10c7328ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "and_another_one['toxic'] = and_another_one[0].apply(set_toxicity)\n",
    "and_another_one.columns = ['comment', 'toxic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8568679-a6ea-4efe-83ce-1a90fa140bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      скотина! что сказать\n",
       "1         я сегодня проезжала по рабочей и между домами ...\n",
       "2         очередной лохотрон. зачем придумывать очередно...\n",
       "3         ретро дежавю ... сложно понять чужое сердце , ...\n",
       "4                   а когда мы статус агрогородка получили?\n",
       "                                ...                        \n",
       "248285                         правильно всё по пять (5)...\n",
       "248286     ёбанные нубы заходите на сервер мой ник _cree...\n",
       "248287    а у меня наверное рекорд в 1962 году в училище...\n",
       "248288                                спасибо всем большое)\n",
       "248289    нельзя ли увеличить хотя бы в два раза некотор...\n",
       "Name: comment, Length: 248290, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_another_one['comment'] = and_another_one['comment'].str.replace('__label__INSULT', '')\n",
    "and_another_one['comment'] = and_another_one['comment'].str.replace('__label__THREAT', '')\n",
    "and_another_one['comment'] = and_another_one['comment'].str.replace('__label__NORMAL ', '')\n",
    "and_another_one['comment'] = and_another_one['comment'].str.replace('__label__OBSCENITY', '')\n",
    "and_another_one['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec1d584-56bc-4ebe-8a47-61192aae9da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 августа поздно вечером нашли вот такую потер...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248284</th>\n",
       "      <td>это евгений леонов,а алексей леонов -космонавт.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248285</th>\n",
       "      <td>правильно всё по пять (5)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248287</th>\n",
       "      <td>а у меня наверное рекорд в 1962 году в училище...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248288</th>\n",
       "      <td>спасибо всем большое)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248289</th>\n",
       "      <td>нельзя ли увеличить хотя бы в два раза некотор...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203685 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  toxic\n",
       "1       я сегодня проезжала по рабочей и между домами ...      0\n",
       "2       очередной лохотрон. зачем придумывать очередно...      0\n",
       "3       ретро дежавю ... сложно понять чужое сердце , ...      0\n",
       "4                 а когда мы статус агрогородка получили?      0\n",
       "5       2 августа поздно вечером нашли вот такую потер...      0\n",
       "...                                                   ...    ...\n",
       "248284    это евгений леонов,а алексей леонов -космонавт.      0\n",
       "248285                       правильно всё по пять (5)...      0\n",
       "248287  а у меня наверное рекорд в 1962 году в училище...      0\n",
       "248288                              спасибо всем большое)      0\n",
       "248289  нельзя ли увеличить хотя бы в два раза некотор...      0\n",
       "\n",
       "[203685 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_another_one.loc[and_another_one['toxic']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828ac27f-60e0-4e45-acac-0c688cbf3524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>скотина! что сказать</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0                               скотина! что сказать    1.0\n",
       "1  я сегодня проезжала по рабочей и между домами ...    0.0\n",
       "2  очередной лохотрон. зачем придумывать очередно...    0.0\n",
       "3  ретро дежавю ... сложно понять чужое сердце , ...    0.0\n",
       "4            а когда мы статус агрогородка получили?    0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([and_another_one, data_another], ignore_index = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5c1b7d-4be9-4863-a71b-d0c3434a1264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262702, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(262702, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53783ac-4799-46a6-be89-ffbda3cf2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^?!.,:а-яё\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "word_count_dict = defaultdict(int)\n",
    "toxic_dict = {}\n",
    "def clean_text(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # dirty words\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\\"\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = special_character_removal.sub('',text)\n",
    "    return text\n",
    "\n",
    "def clean_wylkoi(x):\n",
    "    return re.sub(r'[^\\w\\\\]+', ' ', x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c8663ad-2be2-49af-ad2a-698aa613ec7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>git_comment</th>\n",
       "      <th>my_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>скотина! что сказать</td>\n",
       "      <td>1.0</td>\n",
       "      <td>скотина  !   что сказать</td>\n",
       "      <td>скотина что сказать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>очередной лохотрон  зачем придумывать очередно...</td>\n",
       "      <td>очередной лохотрон зачем придумывать очередной...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ретро дежавю     сложно понять чужое сердце   ...</td>\n",
       "      <td>ретро дежавю сложно понять чужое сердце лиш ощ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>а когда мы статус агрогородка получили ?</td>\n",
       "      <td>а когда мы статус агрогородка получили</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262697</th>\n",
       "      <td>Вонючий совковый скот прибежал и ноет. А вот и...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>вонючий совковый скот прибежал и ноет  а вот и...</td>\n",
       "      <td>вонючий совковый скот прибежал и ноет а вот и ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262698</th>\n",
       "      <td>А кого любить? Гоблина тупорылого что-ли? Или ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>а кого любить ?  гоблина тупорылого что  ли ? ...</td>\n",
       "      <td>а кого любить гоблина тупорылого что ли или ка...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262699</th>\n",
       "      <td>Посмотрел Утомленных солнцем 2. И оказалось, ч...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>посмотрел утомленных солнцем 2  и оказалось  ч...</td>\n",
       "      <td>посмотрел утомленных солнцем 2 и оказалось что...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262700</th>\n",
       "      <td>КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>крымотред нарушает правила раздела т к в нем н...</td>\n",
       "      <td>крымотред нарушает правила раздела т к в нем н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262701</th>\n",
       "      <td>До сих пор пересматриваю его видео. Орамбо кст...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>до сих пор пересматриваю его видео  орамбо кст...</td>\n",
       "      <td>до сих пор пересматриваю его видео орамбо кста...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262702 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  toxic  \\\n",
       "0                                    скотина! что сказать    1.0   \n",
       "1       я сегодня проезжала по рабочей и между домами ...    0.0   \n",
       "2       очередной лохотрон. зачем придумывать очередно...    0.0   \n",
       "3       ретро дежавю ... сложно понять чужое сердце , ...    0.0   \n",
       "4                 а когда мы статус агрогородка получили?    0.0   \n",
       "...                                                   ...    ...   \n",
       "262697  Вонючий совковый скот прибежал и ноет. А вот и...    1.0   \n",
       "262698  А кого любить? Гоблина тупорылого что-ли? Или ...    1.0   \n",
       "262699  Посмотрел Утомленных солнцем 2. И оказалось, ч...    0.0   \n",
       "262700  КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...    1.0   \n",
       "262701  До сих пор пересматриваю его видео. Орамбо кст...    0.0   \n",
       "\n",
       "                                              git_comment  \\\n",
       "0                                скотина  !   что сказать   \n",
       "1       я сегодня проезжала по рабочей и между домами ...   \n",
       "2       очередной лохотрон  зачем придумывать очередно...   \n",
       "3       ретро дежавю     сложно понять чужое сердце   ...   \n",
       "4               а когда мы статус агрогородка получили ?    \n",
       "...                                                   ...   \n",
       "262697  вонючий совковый скот прибежал и ноет  а вот и...   \n",
       "262698  а кого любить ?  гоблина тупорылого что  ли ? ...   \n",
       "262699  посмотрел утомленных солнцем 2  и оказалось  ч...   \n",
       "262700  крымотред нарушает правила раздела т к в нем н...   \n",
       "262701  до сих пор пересматриваю его видео  орамбо кст...   \n",
       "\n",
       "                                               my_comment  \n",
       "0                                     скотина что сказать  \n",
       "1       я сегодня проезжала по рабочей и между домами ...  \n",
       "2       очередной лохотрон зачем придумывать очередной...  \n",
       "3       ретро дежавю сложно понять чужое сердце лиш ощ...  \n",
       "4                 а когда мы статус агрогородка получили   \n",
       "...                                                   ...  \n",
       "262697  вонючий совковый скот прибежал и ноет а вот и ...  \n",
       "262698  а кого любить гоблина тупорылого что ли или ка...  \n",
       "262699  посмотрел утомленных солнцем 2 и оказалось что...  \n",
       "262700  крымотред нарушает правила раздела т к в нем н...  \n",
       "262701  до сих пор пересматриваю его видео орамбо кста...  \n",
       "\n",
       "[262702 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['git_comment'] = df['comment'].apply(clean_text) # git - потому что брал код с гита, но ссылку потерял\n",
    "df['my_comment'] = df['git_comment'].apply(clean_wylkoi)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c995f328-54d5-4476-a95b-d1dd005aa257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я сегодня проезжать по рабочий и между дом снитенко и гомолысова магазин на пустырь бежать кошка похожий окрас мочь я и ошибаться но необычный окрас бросаться в глаз \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(df['my_comment'][1])\n",
    "print(''.join(lemmas))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c3a82f-f15f-4a94-ab19-6de78cc055b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6636daad-ef94-42c7-ae56-e4285ccadfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/262702 [00:13<41:44:08,  1.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_comment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(m\u001b[38;5;241m.\u001b[39mlemmatize(x) ) )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_comment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(m\u001b[38;5;241m.\u001b[39mlemmatize(x) ) )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pymystem3\\mystem.py:265\u001b[0m, in \u001b[0;36mMystem.lemmatize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03mMake morphology analysis for a text and return list of lemmas.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m:rtype:         list\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m need_encode \u001b[38;5;241m=\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m--> 265\u001b[0m infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyze(text)\n\u001b[0;32m    266\u001b[0m lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lemma, infos)))\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_encode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pymystem3\\mystem.py:250\u001b[0m, in \u001b[0;36mMystem.analyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[1;32m--> 250\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_analyze_impl(line))\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\pymystem3\\mystem.py:313\u001b[0m, in \u001b[0;36mMystem._analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_procin\u001b[38;5;241m.\u001b[39mwrite(text)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_procin\u001b[38;5;241m.\u001b[39mwrite(_NL)\n\u001b[1;32m--> 313\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m#obj = json.loads(out)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\subprocess.py:1211\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1208\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1211\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\subprocess.py:1630\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remaining_time(endtime))\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\threading.py:1149\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\threading.py:1169\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[0;32m   1170\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "df['my_comment'].progress_apply(lambda x: ''.join(m.lemmatize(x) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5e6b4-0652-4b48-9669-6b05441477bf",
   "metadata": {},
   "source": [
    "Есть у меня одна любимая история про лемматизации текстов и оптимизацию этого процесса. Существует такая компания маленькая, Яндекс называется, которая иногда выпускает in-public свои не лучшие решения (если что без негатива персонально к яндексоидам, сам там год работал, но тут я правда не понимаю что и почему так, хотя и допускаю, что есть причины на всё описанное ниже).\n",
    "\n",
    "так вот, есть такое решение как pymystem3. Отличная библиотека, которая делает хорошую лемматизацию русских текстов. Но есть нюанс. При её вызове каждый раз инициализируется, какой-то чудовищный по затратам времени, запуск exe файла. Который, уж не знаю почему, совершенно точно нельзя единожды запихнуть в Mystem(). И каждый раз он занимает огромную гору времени (можно посмотреть выше в стопнутую ячейку сколько он предполагает лемматизировать текст dataframe построчно), в то время как каждое предложение на самом деле лемматизируется не больше секунды. \n",
    "\n",
    "И одновременно с этим, если запихнуть все эти тексты в одну строку, в которой оставить между текстами сепаратор типа ' br ', то вся эта вакханалия, займёт не больше пары минут. Потому что сколь бы большим предложение не было, лемматизироваться оно будет в момент.\n",
    "\n",
    "Короче пока корабли ФКН бороздят просторы оптимизации больших нейронных сетей на проде у тысяч айтишников висит вот эти фигня, съедающая немалое количество человеко-часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5055e33-cb4b-4f64-aab3-a770edb6125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExecTimeMystemOneText(texts):\n",
    "    lol = lambda lst, sz: [lst[i:i+sz] for i in range(0, len(lst), sz)]\n",
    "    txtpart = lol(texts, 1000)\n",
    "    res = []\n",
    "    for txtp in tqdm(txtpart):\n",
    "        alltexts = ' '.join([txt + ' br ' for txt in txtp])\n",
    "\n",
    "        words = m.lemmatize(alltexts)\n",
    "        doc = []\n",
    "        for txt in words:\n",
    "            if txt != '\\n' and txt.strip() != '':\n",
    "                if txt == 'br':\n",
    "                    res.append(' '.join(doc))\n",
    "                    doc = []\n",
    "                else:\n",
    "                    doc.append(txt)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe24fbb4-8916-4dfa-b584-8a0a08b558eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 263/263 [04:11<00:00,  1.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>git_comment</th>\n",
       "      <th>my_comment</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>скотина! что сказать</td>\n",
       "      <td>1.0</td>\n",
       "      <td>скотина  !   что сказать</td>\n",
       "      <td>скотина что сказать</td>\n",
       "      <td>скотина что сказать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>я сегодня проезжать по рабочий и между дом сни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>очередной лохотрон  зачем придумывать очередно...</td>\n",
       "      <td>очередной лохотрон зачем придумывать очередной...</td>\n",
       "      <td>очередной лохотрон зачем придумывать очередной...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ретро дежавю     сложно понять чужое сердце   ...</td>\n",
       "      <td>ретро дежавю сложно понять чужое сердце лиш ощ...</td>\n",
       "      <td>ретро дежавю сложно понимать чужой сердце лиш ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>а когда мы статус агрогородка получили ?</td>\n",
       "      <td>а когда мы статус агрогородка получили</td>\n",
       "      <td>а когда мы статус агрогородок получать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262697</th>\n",
       "      <td>Вонючий совковый скот прибежал и ноет. А вот и...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>вонючий совковый скот прибежал и ноет  а вот и...</td>\n",
       "      <td>вонючий совковый скот прибежал и ноет а вот и ...</td>\n",
       "      <td>вонючий совковый скот прибегать и ныть а вот и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262698</th>\n",
       "      <td>А кого любить? Гоблина тупорылого что-ли? Или ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>а кого любить ?  гоблина тупорылого что  ли ? ...</td>\n",
       "      <td>а кого любить гоблина тупорылого что ли или ка...</td>\n",
       "      <td>а кто любить гоблин тупорылый что ли или какой...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262699</th>\n",
       "      <td>Посмотрел Утомленных солнцем 2. И оказалось, ч...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>посмотрел утомленных солнцем 2  и оказалось  ч...</td>\n",
       "      <td>посмотрел утомленных солнцем 2 и оказалось что...</td>\n",
       "      <td>посмотреть утомленный солнце 2 и оказываться ч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262700</th>\n",
       "      <td>КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>крымотред нарушает правила раздела т к в нем н...</td>\n",
       "      <td>крымотред нарушает правила раздела т к в нем н...</td>\n",
       "      <td>крымотред нарушать правило раздел т к в он нет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262701</th>\n",
       "      <td>До сих пор пересматриваю его видео. Орамбо кст...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>до сих пор пересматриваю его видео  орамбо кст...</td>\n",
       "      <td>до сих пор пересматриваю его видео орамбо кста...</td>\n",
       "      <td>до сей пора пересматривать он видео орамбо кст...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262702 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  toxic  \\\n",
       "0                                    скотина! что сказать    1.0   \n",
       "1       я сегодня проезжала по рабочей и между домами ...    0.0   \n",
       "2       очередной лохотрон. зачем придумывать очередно...    0.0   \n",
       "3       ретро дежавю ... сложно понять чужое сердце , ...    0.0   \n",
       "4                 а когда мы статус агрогородка получили?    0.0   \n",
       "...                                                   ...    ...   \n",
       "262697  Вонючий совковый скот прибежал и ноет. А вот и...    1.0   \n",
       "262698  А кого любить? Гоблина тупорылого что-ли? Или ...    1.0   \n",
       "262699  Посмотрел Утомленных солнцем 2. И оказалось, ч...    0.0   \n",
       "262700  КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...    1.0   \n",
       "262701  До сих пор пересматриваю его видео. Орамбо кст...    0.0   \n",
       "\n",
       "                                              git_comment  \\\n",
       "0                                скотина  !   что сказать   \n",
       "1       я сегодня проезжала по рабочей и между домами ...   \n",
       "2       очередной лохотрон  зачем придумывать очередно...   \n",
       "3       ретро дежавю     сложно понять чужое сердце   ...   \n",
       "4               а когда мы статус агрогородка получили ?    \n",
       "...                                                   ...   \n",
       "262697  вонючий совковый скот прибежал и ноет  а вот и...   \n",
       "262698  а кого любить ?  гоблина тупорылого что  ли ? ...   \n",
       "262699  посмотрел утомленных солнцем 2  и оказалось  ч...   \n",
       "262700  крымотред нарушает правила раздела т к в нем н...   \n",
       "262701  до сих пор пересматриваю его видео  орамбо кст...   \n",
       "\n",
       "                                               my_comment  \\\n",
       "0                                     скотина что сказать   \n",
       "1       я сегодня проезжала по рабочей и между домами ...   \n",
       "2       очередной лохотрон зачем придумывать очередной...   \n",
       "3       ретро дежавю сложно понять чужое сердце лиш ощ...   \n",
       "4                 а когда мы статус агрогородка получили    \n",
       "...                                                   ...   \n",
       "262697  вонючий совковый скот прибежал и ноет а вот и ...   \n",
       "262698  а кого любить гоблина тупорылого что ли или ка...   \n",
       "262699  посмотрел утомленных солнцем 2 и оказалось что...   \n",
       "262700  крымотред нарушает правила раздела т к в нем н...   \n",
       "262701  до сих пор пересматриваю его видео орамбо кста...   \n",
       "\n",
       "                                                   lemmas  \n",
       "0                                     скотина что сказать  \n",
       "1       я сегодня проезжать по рабочий и между дом сни...  \n",
       "2       очередной лохотрон зачем придумывать очередной...  \n",
       "3       ретро дежавю сложно понимать чужой сердце лиш ...  \n",
       "4                  а когда мы статус агрогородок получать  \n",
       "...                                                   ...  \n",
       "262697  вонючий совковый скот прибегать и ныть а вот и...  \n",
       "262698  а кто любить гоблин тупорылый что ли или какой...  \n",
       "262699  посмотреть утомленный солнце 2 и оказываться ч...  \n",
       "262700  крымотред нарушать правило раздел т к в он нет...  \n",
       "262701  до сей пора пересматривать он видео орамбо кст...  \n",
       "\n",
       "[262702 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "df['lemmas'] = checkExecTimeMystemOneText(df['my_comment'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19edb60c-922c-402e-8b3f-2b5a07e8375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/radi_hrista_sohrany.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b2daa-8dcc-40e6-aa0e-50304766fb24",
   "metadata": {},
   "source": [
    "# Нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b263b-9e91-48af-b364-406ff15748bd",
   "metadata": {},
   "source": [
    "## Оптимизаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec708534-cee4-4645-ad45-5f4c23d2a6f7",
   "metadata": {},
   "source": [
    "Дальше будет несколько ячеек эволюции MuON, первую версию которого мне скинул Стас. Есть нюанс в том, что запускается она несколько геморно и в то же время у меня возникали с ней проблемы. Нашёл актуальный репозиторий оптимизатора, там всё сильно лучше, но есть второй нюанс. Она работает на распределённых вычислениях, которые хороши, когда у вас много GPU, но вообще не применимо когда у вас одна старая 1060. \n",
    "\n",
    "Короче, даже имея как бы \"готовый\" оптимизатор, причём дважды \"готовый\", мне пришлось его переписывать, чтобы он просто запустился на моей машине. \n",
    "\n",
    "Я **очень** надеюсь, что при этом я не убил никакой логики MuON. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47086beb-41d5-4cc6-8d03-75662a38e18f",
   "metadata": {},
   "source": [
    "### Первая версия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adb7b935-ec12-48e4-a73e-38d087e21da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps):\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = (\n",
    "            b * A + c * A @ A\n",
    "        )  \n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "class Muon_default(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - We believe this optimizer is unlikely to work well for training with small batch size.\n",
    "    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n",
    "\n",
    "    Arguments:\n",
    "        muon_params: The parameters to be optimized by Muon.\n",
    "        lr: The learning rate. The updates will have spectral norm of `lr`. (0.02 is a good default)\n",
    "        momentum: The momentum used by the internal SGD. (0.95 is a good default)\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        ns_steps: The number of Newton-Schulz iterations to run. (6 is probably always enough)\n",
    "        adamw_params: The parameters to be optimized by AdamW. Any parameters in `muon_params` which are\n",
    "        {0, 1}-D or are detected as being the embed or lm_head will be optimized by AdamW as well.\n",
    "        adamw_lr: The learning rate for the internal AdamW.\n",
    "        adamw_betas: The betas for the internal AdamW.\n",
    "        adamw_eps: The epsilon for the internal AdamW.\n",
    "        adamw_wd: The weight decay for the internal AdamW.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "                # RMS-L1 Normalization for the final layer\n",
    "                if \"lm_head.weight\" in p.name:  # Adjust this condition if your final layer has a different name\n",
    "                    w = p.data\n",
    "                    rms = torch.sqrt(torch.mean(w**2))\n",
    "                    l1 = torch.sum(torch.abs(w))\n",
    "                    p.data = w / (rms + 1e-8)  # Add a small epsilon to avoid division by zero\n",
    "                    p.data = p.data / (torch.sum(torch.abs(p.data)) + 1e-8)\n",
    "\n",
    "\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def get_optimizer(optimizer_name, model, lr=1e-3, wd=0.1):\n",
    "        if optimizer_name == \"adamw\":\n",
    "            return torch.optim.AdamW(\n",
    "                model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.95)\n",
    "            )\n",
    "        elif optimizer_name == \"muon\":\n",
    "            muon_params = [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "            ]\n",
    "            adamw_params = [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if not (\n",
    "                    p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "                )\n",
    "            ]\n",
    "    \n",
    "            return Muon_default(\n",
    "                lr=lr,\n",
    "                wd=wd,\n",
    "                muon_params=muon_params,\n",
    "                adamw_params=adamw_params,\n",
    "            )\n",
    "\n",
    "        elif optimizer_name == \"muon_l1\":\n",
    "            muon_params = [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "            ]\n",
    "            adamw_params = [\n",
    "                p\n",
    "                for name, p in model.named_parameters()\n",
    "                if not (\n",
    "                    p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "                )\n",
    "            ]\n",
    "    \n",
    "            return Muon(\n",
    "                lr=lr,\n",
    "                wd=wd,\n",
    "                muon_params=muon_params,\n",
    "                adamw_params=adamw_params,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            assert 0, \"optimizer not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196d977-d2b2-4c15-ae66-c4fd53319148",
   "metadata": {},
   "source": [
    "### Более новая версия MuON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6458caf-5992-477d-8124-d865db9878d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import Tensor\n",
    "\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "    \n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate used by the internal SGD.\n",
    "        momentum: The momentum used by the internal SGD.\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        ns_steps: The number of Newton-Schulz iteration steps to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=None, world_size=None):\n",
    "        if (rank is None) or (world_size is None):\n",
    "            raise Exception(\"world_size and rank params required, if you want to use this optimizer on a single GPU, pass rank=0 and world_size=1.\")\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        params: list[Tensor] = [*params]\n",
    "        param_groups = []\n",
    "        for size in {p.numel() for p in params}:\n",
    "            b = torch.empty(world_size, size, dtype=torch.bfloat16, device=\"cuda\")\n",
    "            group = dict(params=[p for p in params if p.numel() == size],\n",
    "                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])\n",
    "            param_groups.append(group)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            update_buffer: Tensor = group[\"update_buffer\"]\n",
    "            update_buffer_views: list[Tensor] = group[\"update_buffer_views\"]\n",
    "            # generate weight updates in distributed fashion\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            handle = None\n",
    "            params_world = None\n",
    "            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng\n",
    "                handle.wait()\n",
    "                for p_world, g_world in zip(params_world, update_buffer_views):\n",
    "                    p_world.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p_world.add_(g_world.view_as(p_world),\n",
    "                                 alpha=-group[\"lr\"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)\n",
    "            for base_i in range(len(params))[::self.world_size]:\n",
    "                if base_i + self.rank < len(params):\n",
    "                    p = params[base_i + self.rank]\n",
    "                    g = p.grad\n",
    "                    assert g is not None\n",
    "                    state = self.state[p]\n",
    "                    if \"momentum_buffer\" not in state:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                    buf: Tensor = state[\"momentum_buffer\"]\n",
    "                    buf.lerp_(g, 1 - group[\"momentum\"])\n",
    "                    g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "                    if g.ndim == 4: # for the case of conv filters\n",
    "                        g = g.view(len(g), -1)\n",
    "                    g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"]).flatten()\n",
    "                else:\n",
    "                    g = update_buffer_views[self.rank]\n",
    "                if base_i > 0:\n",
    "                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng\n",
    "                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)\n",
    "                params_world = params[base_i : base_i + self.world_size]\n",
    "            update_prev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e72d44-ca05-4a51-a6b3-8375be061f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58cab587-b768-440f-9cf6-575bd3a9d537",
   "metadata": {},
   "source": [
    "### rewriten without distributed computations. \n",
    "\n",
    "моя уверенность, что я не проебал нигде логику изначального оптимизатора: молюсь об этом перед сном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13ed5ddd-6c63-4a1d-8fb9-2bf2712c4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import Tensor\n",
    "\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "    \n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate used by the internal SGD.\n",
    "        momentum: The momentum used by the internal SGD.\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        ns_steps: The number of Newton-Schulz iteration steps to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.param_groups[0]['params']:\n",
    "            g = p.grad\n",
    "            assert g is not None\n",
    "            state = self.state[p]\n",
    "            if \"momentum_buffer\" not in state:\n",
    "                state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "            buf: Tensor = state[\"momentum_buffer\"]\n",
    "            buf.lerp_(g, 1 - self.param_groups[0][\"momentum\"])\n",
    "            g = g.lerp_(buf, self.param_groups[0][\"momentum\"]) if self.param_groups[0][\"nesterov\"] else buf\n",
    "            if g.ndim == 4: # for the case of conv filters\n",
    "                g = g.view(len(g), -1)\n",
    "            g = zeropower_via_newtonschulz5(g, steps=self.param_groups[0][\"ns_steps\"]).flatten()\n",
    "            p.mul_(1 - self.param_groups[0][\"lr\"] * self.param_groups[0][\"weight_decay\"])\n",
    "            p.add_(g.view_as(p),\n",
    "                   alpha=-self.param_groups[0][\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f123b6-f6f5-40aa-8df4-03f963ba86b2",
   "metadata": {},
   "source": [
    "### Prep to neural network train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b19bef-333c-42d0-bf55-23340bfda0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = df[['lemmas', 'toxic']].copy()  \n",
    "\n",
    "pretrained tokenizer taken. Honestly randomest choice in my life\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_dataset = MoonDataset(\n",
    "    texts=train_df[\"lemmas\"].tolist(),\n",
    "    labels=train_df[\"toxic\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    ")\n",
    "test_dataset = MoonDataset(\n",
    "    texts=test_df[\"lemmas\"].tolist(),\n",
    "    labels=test_df[\"toxic\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Super simple try - doesn`t give significant accuracy growth  throug train\n",
    "# class TextClassifier(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, max_length):\n",
    "#         super(TextClassifier, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         _, h_n = self.rnn(x)\n",
    "#         h_n = h_n.squeeze(0)\n",
    "#         output = self.fc(h_n)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8dc03994-bae3-4ecd-9305-21b03a4e4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented code from prev iterations of war with MuON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1714a-952b-489b-89ca-be0fd5df2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.distributed as dist\n",
    "# import os\n",
    "# os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# os.environ['MASTER_PORT'] = '12355'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"USE_LIBUV\"] = \"0\"\n",
    "# dist.init_process_group(backend='nccl', init_method='env://', rank = torch.cuda.device_count(), world_size = 1)\n",
    "# Initialize the process group\n",
    "\n",
    "# dist.init_process_group(backend=\"gloo\", init_method=\"env://\", rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2f0ea-c7a4-4fe0-9a23-07d507087c17",
   "metadata": {},
   "source": [
    "## MuON classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19b0fa49-d82b-4908-a2f0-7b38414986ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Ua = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Va = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        scores = self.Va(torch.tanh(self.Wa(hidden_states) + self.Ua(hidden_states)))\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), hidden_states).squeeze(1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, max_length, dropout_rate=0.5):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_rate)\n",
    "        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        context_vector, attention_weights = self.attention(x)\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_preds = []\n",
    "        epoch_train_labels = []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            # for opt in optimizer:\n",
    "            #     opt.zero_grad()\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            for opt in optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            # Collect metrics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            epoch_train_preds.extend(predicted.cpu().numpy())\n",
    "            epoch_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        epoch_train_accuracy = accuracy_score(epoch_train_labels, epoch_train_preds)\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        epoch_test_preds = []\n",
    "        epoch_test_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                epoch_test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                epoch_test_preds.extend(predicted.cpu().numpy())\n",
    "                epoch_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_test_accuracy = accuracy_score(epoch_test_labels, epoch_test_preds)\n",
    "        epoch_test_loss /= len(test_loader)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {epoch_test_loss:.4f}, Test Accuracy: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# hyperpams\n",
    "vocab_size = 119547  \n",
    "hidden_dim = 256\n",
    "num_classes = 2  \n",
    "max_length = 256\n",
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = .001\n",
    "weight_decay = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = TextClassifier(vocab_size, embed_dim, hidden_dim, num_classes, max_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "muon_params = [p for p in model.rnn.parameters() if p.ndim >= 2]\n",
    "adamw_params = ([p for p in model.rnn.parameters() if p.ndim < 2]\n",
    "              + list(model.embedding.parameters())\n",
    "              + list(model.attention.parameters())\n",
    "              + list(model.fc.parameters())\n",
    "              + list(model.dropout.parameters()))\n",
    "\n",
    "mu_opti = Muon(muon_params, lr=0.02, momentum=0.95)\n",
    "\n",
    "optimizers = [\n",
    "    mu_opti,\n",
    "    torch.optim.AdamW(adamw_params, lr=learning_rate, betas=(0.90, 0.95), weight_decay=0.01)\n",
    "]\n",
    "\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5bbea089-6efc-46b6-be8c-100d6e2438c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:16<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Train Loss: 0.4601, Train Accuracy: 0.8152\n",
      "Test Loss: 0.4581, Test Accuracy: 0.8114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:11<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "Train Loss: 0.4750, Train Accuracy: 0.8099\n",
      "Test Loss: 0.4654, Test Accuracy: 0.8104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:12<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:\n",
      "Train Loss: 0.4769, Train Accuracy: 0.8102\n",
      "Test Loss: 0.4572, Test Accuracy: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:16<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:\n",
      "Train Loss: 0.4808, Train Accuracy: 0.8099\n",
      "Test Loss: 0.4729, Test Accuracy: 0.8103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:14<00:00, 17.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:\n",
      "Train Loss: 0.4821, Train Accuracy: 0.8095\n",
      "Test Loss: 0.4698, Test Accuracy: 0.8101\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the mode\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    model, train_loader, test_loader, optimizers, criterion, num_epochs, device\n",
    ")\n",
    "\n",
    "# Plot results (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57b00a29-933c-44f4-bda5-c0366a18475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_losses': [0.45807239883006456,\n",
       "  0.4653738799928058,\n",
       "  0.4572181827167501,\n",
       "  0.47291148290373886,\n",
       "  0.46977429142410115],\n",
       " 'train_losses': [0.4600604398555821,\n",
       "  0.4750154730892407,\n",
       "  0.47688496447590545,\n",
       "  0.4807783738392668,\n",
       "  0.48205044148527765],\n",
       " 'train_accuracies': [0.8151702742183373,\n",
       "  0.8099361917767807,\n",
       "  0.8101550715879731,\n",
       "  0.8098695761820699,\n",
       "  0.8094698826138056],\n",
       " 'test_accuracies': [0.8114424925296435,\n",
       "  0.8103766582288118,\n",
       "  0.8118992786585714,\n",
       "  0.8103005272073238,\n",
       "  0.8101292324089758]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muon_simple = {\n",
    "               'test_losses': test_losses,\n",
    "               'train_losses':train_losses,\n",
    "               'train_accuracies':train_accuracies,\n",
    "               'test_accuracies':  test_accuracies\n",
    "              } \n",
    "\n",
    "muon_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08280d71-67dd-4594-84f2-a3614b98a18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_losses': [0.4961309000922324, 0.4950909514284163, 0.49871998202695483],\n",
       " 'train_losses': [0.49204724586844684, 0.4918178864429477, 0.4917184129838899],\n",
       " 'train_accuracies': [0.812467584375788, 0.812467584375788, 0.812467584375788],\n",
       " 'test_accuracies': [0.80931082392798, 0.80931082392798, 0.80931082392798]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988169c5-75b6-4e88-bf0d-3f6aee2c9ac3",
   "metadata": {},
   "source": [
    "## ADAMW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "337cb49b-0bf1-49f7-9097-1919ca0df2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Ua = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Va = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        scores = self.Va(torch.tanh(self.Wa(hidden_states) + self.Ua(hidden_states)))\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), hidden_states).squeeze(1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, max_length, dropout_rate=0.5):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_rate)\n",
    "        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        context_vector, attention_weights = self.attention(x)\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_preds = []\n",
    "        epoch_train_labels = []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            # for opt in optimizer:\n",
    "            #     opt.zero_grad()\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            for opt in optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            # Collect metrics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            epoch_train_preds.extend(predicted.cpu().numpy())\n",
    "            epoch_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        epoch_train_accuracy = accuracy_score(epoch_train_labels, epoch_train_preds)\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        epoch_test_preds = []\n",
    "        epoch_test_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                epoch_test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                epoch_test_preds.extend(predicted.cpu().numpy())\n",
    "                epoch_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_test_accuracy = accuracy_score(epoch_test_labels, epoch_test_preds)\n",
    "        epoch_test_loss /= len(test_loader)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {epoch_test_loss:.4f}, Test Accuracy: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# hyperpams\n",
    "vocab_size = 119547  \n",
    "hidden_dim = 256\n",
    "num_classes = 2  \n",
    "max_length = 256\n",
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = .001\n",
    "weight_decay = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = TextClassifier(vocab_size, embed_dim, hidden_dim, num_classes, max_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizers = [torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.90, 0.95), weight_decay=0.01)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cdfc1cf5-ba4f-4775-a41c-ab4b6a2939d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [19:40<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Train Loss: 0.1624, Train Accuracy: 0.9458\n",
      "Test Loss: 0.1786, Test Accuracy: 0.9476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [19:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "Train Loss: 0.1311, Train Accuracy: 0.9585\n",
      "Test Loss: 0.1465, Test Accuracy: 0.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [19:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:\n",
      "Train Loss: 0.1311, Train Accuracy: 0.9586\n",
      "Test Loss: 0.1327, Test Accuracy: 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [20:41<00:00, 21.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:\n",
      "Train Loss: 0.1305, Train Accuracy: 0.9582\n",
      "Test Loss: 0.1373, Test Accuracy: 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [20:46<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:\n",
      "Train Loss: 0.1340, Train Accuracy: 0.9577\n",
      "Test Loss: 0.1788, Test Accuracy: 0.9494\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    model, train_loader, test_loader, optimizers, criterion, num_epochs, device\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ddd247d-b6c5-4b66-97fb-f52c1618e8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_losses': [0.17855756451699364,\n",
       "  0.14651074925603455,\n",
       "  0.13266612403094769,\n",
       "  0.13725727271281826,\n",
       "  0.17878969133820907],\n",
       " 'train_losses': [0.16241411839547845,\n",
       "  0.13109724776081513,\n",
       "  0.1311368783892389,\n",
       "  0.13053733161880074,\n",
       "  0.13396294315209992],\n",
       " 'train_accuracies': [0.9457939389325327,\n",
       "  0.9585127592655155,\n",
       "  0.9586317156846418,\n",
       "  0.9582082308325521,\n",
       "  0.9576515147910412],\n",
       " 'test_accuracies': [0.9475647589501532,\n",
       "  0.9581469709369825,\n",
       "  0.9572714641898707,\n",
       "  0.9572714641898707,\n",
       "  0.9494109362212367]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adamw_ = {\n",
    "               'test_losses': test_losses,\n",
    "               'train_losses':train_losses,\n",
    "               'train_accuracies':train_accuracies,\n",
    "               'test_accuracies':  test_accuracies\n",
    "              } \n",
    "\n",
    "adamw_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d08a5-de42-4845-ba56-58e95ef4c804",
   "metadata": {},
   "source": [
    "## MuON  L1->RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "68617c4a-45d3-4be0-ae03-460db1ded032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def zeropower_via_newtonschulz5_1(G: Tensor, steps: int) -> Tensor:\n",
    "\n",
    "    assert G.ndim >= 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "\n",
    "    l1_norm = torch.linalg.norm(X, ord=1, dim=(-2, -1), keepdim=True) \n",
    "    X = X / (l1_norm + 1e-7)\n",
    "    \n",
    "    rms = torch.sqrt(torch.mean(X**2, dim=(-2, -1), keepdim=True))\n",
    "    rms = torch.clamp(rms, min=1e-7, max=1e7)  # clip the RMS norm\n",
    "    X = X / (rms + 1e-7)\n",
    "\n",
    "    X = torch.clamp(X, min=-1e6, max=1e6)  # clip the values\n",
    "\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X\n",
    "\n",
    "class Muon_l1(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.param_groups[0]['params']:\n",
    "            g = p.grad\n",
    "            assert g is not None\n",
    "            state = self.state[p]\n",
    "            if \"momentum_buffer\" not in state:\n",
    "                state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "            buf: Tensor = state[\"momentum_buffer\"]\n",
    "            buf.lerp_(g, 1 - self.param_groups[0][\"momentum\"])\n",
    "            g = g.lerp_(buf, self.param_groups[0][\"momentum\"]) if self.param_groups[0][\"nesterov\"] else buf\n",
    "            if g.ndim == 4: # for the case of conv filters\n",
    "                g = g.view(len(g), -1)\n",
    "            g = zeropower_via_newtonschulz5_1(g, steps=self.param_groups[0][\"ns_steps\"]).flatten()\n",
    "            p.mul_(1 - self.param_groups[0][\"lr\"] * self.param_groups[0][\"weight_decay\"])\n",
    "            p.add_(g.view_as(p),\n",
    "                   alpha=-self.param_groups[0][\"lr\"] * max(1, p.size(-2) / p.size(-1) + 1e-7)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cdd2227d-12ac-438d-880d-69cee23e5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Ua = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Va = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        scores = self.Va(torch.tanh(self.Wa(hidden_states) + self.Ua(hidden_states)))\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), hidden_states).squeeze(1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, max_length, dropout_rate=0.5):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_rate)\n",
    "        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        context_vector, attention_weights = self.attention(x)\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_preds = []\n",
    "        epoch_train_labels = []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            # for opt in optimizer:\n",
    "            #     opt.zero_grad()\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            for opt in optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Collect metrics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            epoch_train_preds.extend(predicted.cpu().numpy())\n",
    "            epoch_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "        epoch_train_accuracy = accuracy_score(epoch_train_labels, epoch_train_preds)\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        epoch_test_preds = []\n",
    "        epoch_test_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                epoch_test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                epoch_test_preds.extend(predicted.cpu().numpy())\n",
    "                epoch_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_test_accuracy = accuracy_score(epoch_test_labels, epoch_test_preds)\n",
    "        epoch_test_loss /= len(test_loader)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {epoch_test_loss:.4f}, Test Accuracy: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# hyperpams\n",
    "vocab_size = 119547  \n",
    "hidden_dim = 256\n",
    "num_classes = 2  \n",
    "max_length = 256\n",
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = .001\n",
    "weight_decay = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize\n",
    "model = TextClassifier(vocab_size, embed_dim, hidden_dim, num_classes, max_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "muon_params = [p for p in model.rnn.parameters() if p.ndim >= 2]\n",
    "adamw_params = ([p for p in model.rnn.parameters() if p.ndim < 2]\n",
    "              + list(model.embedding.parameters())\n",
    "              + list(model.attention.parameters())\n",
    "              + list(model.fc.parameters())\n",
    "              + list(model.dropout.parameters()))\n",
    "\n",
    "mu_opti = Muon_l1(muon_params, lr=0.02, momentum=0.95)\n",
    "\n",
    "optimizers = [\n",
    "    mu_opti,\n",
    "    torch.optim.AdamW(adamw_params, lr=learning_rate, betas=(0.90, 0.95), weight_decay=0.01)\n",
    "]\n",
    "\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "362f065a-39fd-49b4-9157-8b661f571713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:05<00:00, 17.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Train Loss: 0.4559, Train Accuracy: 0.8165\n",
      "Test Loss: 0.4478, Test Accuracy: 0.8156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:06<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "Train Loss: 0.4739, Train Accuracy: 0.8101\n",
      "Test Loss: 0.4590, Test Accuracy: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:09<00:00, 17.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:\n",
      "Train Loss: 0.4749, Train Accuracy: 0.8107\n",
      "Test Loss: 0.4791, Test Accuracy: 0.8008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:09<00:00, 17.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:\n",
      "Train Loss: 0.4772, Train Accuracy: 0.8100\n",
      "Test Loss: 0.4693, Test Accuracy: 0.8098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [25:09<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:\n",
      "Train Loss: 0.4808, Train Accuracy: 0.8099\n",
      "Test Loss: 0.4602, Test Accuracy: 0.8061\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    model, train_loader, test_loader, optimizers, criterion, num_epochs, device\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d485b13e-4391-46a1-b128-c7b4efd200de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_losses': [0.4478307020556676,\n",
       "  0.45900218879370686,\n",
       "  0.4791118669438195,\n",
       "  0.4692504374451037,\n",
       "  0.4602013078226677],\n",
       " 'train_losses': [0.45591453960079803,\n",
       "  0.473868304338407,\n",
       "  0.47486341524924364,\n",
       "  0.4772038891794378,\n",
       "  0.4807529138573937],\n",
       " 'train_accuracies': [0.8164550035449013,\n",
       "  0.8101360385609129,\n",
       "  0.8106737215753637,\n",
       "  0.8100170821417866,\n",
       "  0.8099076422361904],\n",
       " 'test_accuracies': [0.8155535676899944,\n",
       "  0.8120325079461753,\n",
       "  0.8008031822766982,\n",
       "  0.80978664281228,\n",
       "  0.8060562227593688]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muon_l1_rms = {\n",
    "               'test_losses': test_losses,\n",
    "               'train_losses':train_losses,\n",
    "               'train_accuracies':train_accuracies,\n",
    "               'test_accuracies':  test_accuracies\n",
    "              } \n",
    "\n",
    "muon_l1_rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1388c09-53bb-4177-ba6a-4b2cbcf9eb25",
   "metadata": {},
   "source": [
    "## Важный эксперимент"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327393ef-b1d7-4819-8c89-db6579eb055f",
   "metadata": {},
   "source": [
    "Вообще я тут ещё посидел покумекал над оптимизатором и там чтобы затестить уменьшил размер нейронной сети... захочу может отдельно обсудить, тут не успеет отработать, но первичные результаты сильно лучше MuON выше. Выводы в презе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4acc5ccd-346a-44a0-be32-ff377174fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def zeropower_via_newtonschulz5_1(G: Tensor, steps: int) -> Tensor:\n",
    "\n",
    "    assert G.ndim >= 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "\n",
    "    l1_norm = X.abs().sum(dim=(-2, -1), keepdim=True) + 1e-7\n",
    "    # Normalize by L1 norm\n",
    "    X = X / l1_norm\n",
    "    # Scale to ensure RMS is 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X\n",
    "\n",
    "class Muon_l1(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.param_groups[0]['params']:\n",
    "            g = p.grad\n",
    "            assert g is not None\n",
    "            state = self.state[p]\n",
    "            if \"momentum_buffer\" not in state:\n",
    "                state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "            buf: torch.Tensor = state[\"momentum_buffer\"]\n",
    "            buf.lerp_(g, 1 - self.param_groups[0][\"momentum\"])\n",
    "            g = g.lerp_(buf, self.param_groups[0][\"momentum\"]) if self.param_groups[0][\"nesterov\"] else buf\n",
    "\n",
    "            if g.ndim == 4:  # for the case of conv filters\n",
    "                g = g.view(len(g), -1)\n",
    "\n",
    "            # Check for NaN values in the gradient before normalization\n",
    "            if torch.isnan(g).any():\n",
    "                print(\"NaN values detected in gradient before normalization\")\n",
    "                continue\n",
    "\n",
    "            g = zeropower_via_newtonschulz5_1(g, steps=self.param_groups[0][\"ns_steps\"]).flatten()\n",
    "\n",
    "            # Check for NaN values in the gradient after normalization\n",
    "            if torch.isnan(g).any():\n",
    "                print(\"NaN values detected in gradient after normalization\")\n",
    "                continue\n",
    "\n",
    "            p.mul_(1 - self.param_groups[0][\"lr\"] * self.param_groups[0][\"weight_decay\"])\n",
    "            p.add_(g.view_as(p),\n",
    "                   alpha=-self.param_groups[0][\"lr\"] * max(1, p.size(-2) / p.size(-1) + 1e-7)**0.5)\n",
    "\n",
    "            # Check for NaN values in the parameters after update\n",
    "            if torch.isnan(p).any():\n",
    "                print(\"NaN values detected in parameters after update\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5b513c25-26fd-4c98-99b3-45f69ce63e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Ua = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Va = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        scores = self.Va(torch.tanh(self.Wa(hidden_states) + self.Ua(hidden_states)))\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), hidden_states).squeeze(1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, max_length, dropout_rate=0.5):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_rate)\n",
    "        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        context_vector, attention_weights = self.attention(x)\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_preds = []\n",
    "        epoch_train_labels = []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            # for opt in optimizer:\n",
    "            #     opt.zero_grad()\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            for opt in optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Collect metrics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            epoch_train_preds.extend(predicted.cpu().numpy())\n",
    "            epoch_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "        epoch_train_accuracy = accuracy_score(epoch_train_labels, epoch_train_preds)\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        epoch_test_preds = []\n",
    "        epoch_test_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                epoch_test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                epoch_test_preds.extend(predicted.cpu().numpy())\n",
    "                epoch_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_test_accuracy = accuracy_score(epoch_test_labels, epoch_test_preds)\n",
    "        epoch_test_loss /= len(test_loader)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {epoch_test_loss:.4f}, Test Accuracy: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# hyperpams\n",
    "vocab_size = 119547  \n",
    "hidden_dim = 16\n",
    "num_classes = 2  \n",
    "max_length = 256\n",
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = .001\n",
    "weight_decay = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize\n",
    "model = TextClassifier(vocab_size, embed_dim, hidden_dim, num_classes, max_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "muon_params = [p for p in model.rnn.parameters() if p.ndim >= 2]\n",
    "adamw_params = ([p for p in model.rnn.parameters() if p.ndim < 2]\n",
    "              + list(model.embedding.parameters())\n",
    "              + list(model.attention.parameters())\n",
    "              + list(model.fc.parameters())\n",
    "              + list(model.dropout.parameters()))\n",
    "\n",
    "mu_opti = Muon_l1(muon_params, lr=0.001, momentum=0.95)\n",
    "\n",
    "optimizers = [\n",
    "    mu_opti,\n",
    "    torch.optim.AdamW(adamw_params, lr=learning_rate, betas=(0.90, 0.95), weight_decay=0.01)\n",
    "]\n",
    "\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b682956e-9445-41b1-8a97-2ff0f2ffbe0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [09:02<00:00, 48.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Train Loss: 0.1690, Train Accuracy: 0.9393\n",
      "Test Loss: 0.1285, Test Accuracy: 0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26271/26271 [09:47<00:00, 44.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "Train Loss: 0.1213, Train Accuracy: 0.9589\n",
      "Test Loss: 0.1232, Test Accuracy: 0.9589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 175/26271 [00:03<08:10, 53.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, test_losses, train_accuracies, test_accuracies \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m      2\u001b[0m     model, train_loader, test_loader, optimizers, criterion, num_epochs, device\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[123], line 60\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, optimizer, criterion, num_epochs, device)\u001b[0m\n\u001b[0;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizer:\n\u001b[1;32m---> 60\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Collect metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[117], line 53\u001b[0m, in \u001b[0;36mMuon_l1.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN values detected in gradient before normalization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m g \u001b[38;5;241m=\u001b[39m zeropower_via_newtonschulz5_1(g, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Check for NaN values in the gradient after normalization\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(g)\u001b[38;5;241m.\u001b[39many():\n",
      "Cell \u001b[1;32mIn[117], line 14\u001b[0m, in \u001b[0;36mzeropower_via_newtonschulz5_1\u001b[1;34m(G, steps)\u001b[0m\n\u001b[0;32m     12\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m/\u001b[39m l1_norm\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Scale to ensure RMS is 1\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m/\u001b[39m (X\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m     17\u001b[0m     A \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m X\u001b[38;5;241m.\u001b[39mmT\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\_tensor.py:872\u001b[0m, in \u001b[0;36mTensor.norm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    870\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mnorm, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, p\u001b[38;5;241m=\u001b[39mp, dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mkeepdim, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[0;32m    871\u001b[0m     )\n\u001b[1;32m--> 872\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m, p, dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\functional.py:1805\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   1802\u001b[0m     dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1803\u001b[0m ):\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1805\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m   1806\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[0;32m   1807\u001b[0m         )\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m   1810\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout\n\u001b[0;32m   1811\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    model, train_loader, test_loader, optimizers, criterion, num_epochs, device\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975a679-701f-4847-8a2f-51f26d1bfadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Случайно обнулил ячейку, в презе чуть другие цифры, потому что с первого прохода остались. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babdf96-cf3e-4a8a-8a92-32ff22febd01",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5b1e9-8a36-4b8b-986b-21b78d26be15",
   "metadata": {},
   "source": [
    "ADAMW лучше себя показывает. \n",
    "\n",
    "Однако по последней экспериментальной ячейке есть подозрения, что причиной может быть архитектура сети. Или же выбранные гиперпараметры. Надо бы поисследовать ещё."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b3ed0-75e8-4e10-ad29-a86c8df1373a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "356fc02b-de43-463d-b310-cf5f8aa612f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fed92a1-adad-4eb1-a292-012f9c5e4be6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d57ff0d-615f-4716-9a19-5fb65d897c9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23083527-731f-427b-b86b-2f79e63b197c",
   "metadata": {},
   "source": [
    "# Старая попытка потренить LLM из репозитория MuON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88977026-e82a-4eae-81e3-3cc1683e75e0",
   "metadata": {},
   "source": [
    "### ВСЕ ЧТО НИЖЕ ОСТАВЛЕНО ТОЛЬКО ДЛЯ ДЕМОНСТРАЦИИ, ЧТО Я ПИТЫАЛСЯ И LLM ПОТРЕНИТЬ НА СВОИХ РЕСУРСАХ.\n",
    "\n",
    "Упёрся в очень долгое обучение. Смотреть на предмет решений для оценки не нужно. Если по каким-то неведомым мне причинам хочется, то прошу делать строго под \n",
    "\n",
    "https://music.yandex.ru/album/30651393/track/124684206?utm_source=web&utm_medium=copy_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965392e3-f6d8-4515-adc4-adaf7783293b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4bfe32c-079f-4aa2-86e9-98f8fbde2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonDataset(Dataset):\n",
    "    def __init__(self, dataset_name, dataset, tokenizer, max_length=256):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = dataset[\"train\"][\"text\"]\n",
    "        self.max_length = max_length\n",
    "        self.tokens = []\n",
    "        self._tokenize_texts()\n",
    "\n",
    "    def _tokenize_texts(self):\n",
    "        if os.path.exists(f\"{self.dataset_name}.bin\"):\n",
    "            self.tokens = torch.load(f\"{self.dataset_name}.bin\")\n",
    "        else:\n",
    "            for text in tqdm(self.texts, desc=\"Tokenizing texts\"):\n",
    "                encoded = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "                self.tokens.extend(encoded)\n",
    "            torch.save(self.tokens, f\"{self.dataset_name}.bin\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * (self.max_length)\n",
    "        end_idx = start_idx + (self.max_length)\n",
    "        token_slice = self.tokens[start_idx:end_idx]\n",
    "        data = torch.tensor(token_slice, dtype=torch.long)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c88c1b9-2ec3-4913-aef1-12fd20168d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is a modified version adapted from the following GitHub repository:\n",
    "# https://github.com/KellerJordan/Muon/blob/master/muon.py\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = (\n",
    "            b * A + c * A @ A\n",
    "        )  # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec2b03-b548-4281-aea7-be89c6e32696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is a modified version adapted from the following GitHub repository:\n",
    "# https://github.com/KellerJordan/Muon/blob/master/muon.py\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = (\n",
    "            b * A + c * A @ A\n",
    "        )  # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "class Muon_default(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "558c17e1-51c8-4c69-a624-900b7bfe31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - We believe this optimizer is unlikely to work well for training with small batch size.\n",
    "    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n",
    "\n",
    "    Arguments:\n",
    "        muon_params: The parameters to be optimized by Muon.\n",
    "        lr: The learning rate. The updates will have spectral norm of `lr`. (0.02 is a good default)\n",
    "        momentum: The momentum used by the internal SGD. (0.95 is a good default)\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        ns_steps: The number of Newton-Schulz iterations to run. (6 is probably always enough)\n",
    "        adamw_params: The parameters to be optimized by AdamW. Any parameters in `muon_params` which are\n",
    "        {0, 1}-D or are detected as being the embed or lm_head will be optimized by AdamW as well.\n",
    "        adamw_lr: The learning rate for the internal AdamW.\n",
    "        adamw_betas: The betas for the internal AdamW.\n",
    "        adamw_eps: The epsilon for the internal AdamW.\n",
    "        adamw_wd: The weight decay for the internal AdamW.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "                # RMS-L1 Normalization for the final layer\n",
    "                if \"lm_head.weight\" in p.name:  # Adjust this condition if your final layer has a different name\n",
    "                    w = p.data\n",
    "                    rms = torch.sqrt(torch.mean(w**2))\n",
    "                    l1 = torch.sum(torch.abs(w))\n",
    "                    p.data = w / (rms + 1e-8)  # Add a small epsilon to avoid division by zero\n",
    "                    p.data = p.data / (torch.sum(torch.abs(p.data)) + 1e-8)\n",
    "\n",
    "\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b12e6d4-d33b-45cb-bfb7-8049dd2fc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon_default(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7568707-66ee-4b15-9360-597f4bd2dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_dataloader(model_name, dataset_name, hidden_size):\n",
    "    name2path = {\n",
    "        \"openwebtext-100k\": \"Elriggs/openwebtext-100k\",\n",
    "    }\n",
    "    train_dataset = load_dataset(name2path[dataset_name], trust_remote_code=True)\n",
    "    if model_name == \"qwen\":\n",
    "        tokenizer = Qwen2Tokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-0.5B\", trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        assert 0, f\"model {model_name} not supported\"\n",
    "    train_dataset = MoonDataset(dataset_name, train_dataset, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    if model_name == \"qwen\":\n",
    "        config = Qwen2Config(\n",
    "            attention_dropout=0.0,\n",
    "            bos_token_id=151643,\n",
    "            eos_token_id=151643,\n",
    "            hidden_act=\"silu\",\n",
    "            hidden_size=hidden_size,\n",
    "            initializer_range=0.02,\n",
    "            intermediate_size=2048, # Reduce from 4864,\n",
    "            max_position_embeddings=513,\n",
    "            max_window_layers=12,\n",
    "            model_type=\"qwen2\",\n",
    "            num_hidden_layers=6,  # Reduce from 12\n",
    "            num_attention_heads=8,  # Reduce from 16\n",
    "            num_key_value_heads=16,\n",
    "            rms_norm_eps=1e-06,\n",
    "            rope_theta=1000000.0,\n",
    "            sliding_window=1024,\n",
    "            tie_word_embeddings=True,\n",
    "            torch_dtype=\"bfloat16\",\n",
    "            use_cache=True,\n",
    "            use_mrope=False,\n",
    "            use_sliding_window=False,\n",
    "            vocab_size=151936,\n",
    "        )\n",
    "        model = Qwen2ForCausalLM(config)\n",
    "    else:\n",
    "        assert 0, f\"model {model_name} not supported\"\n",
    "    return model, train_loader\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name, model, lr=1e-3, wd=0.1):\n",
    "    if optimizer_name == \"adamw\":\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.95)\n",
    "        )\n",
    "    elif optimizer_name == \"muon\":\n",
    "        muon_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "        ]\n",
    "        adamw_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if not (\n",
    "                p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return Muon(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            muon_params=muon_params,\n",
    "            adamw_params=adamw_params,\n",
    "        )\n",
    "    else:\n",
    "        assert 0, \"optimizer not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf92cb-5780-4fc7-8b90-20f64c223752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b53832d-8040-4b6c-992a-4c6e2625944a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6: Configure training parameters\n",
    "\n",
    "model_name = \"qwen\"  \n",
    "optimizer_name = \"muon\"  # Options: \"adamw\", \"muon\"\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.1\n",
    "dataset_name = \"openwebtext-100k\"\n",
    "hidden_size = 512\n",
    "\n",
    "# Setup logging\n",
    "logger.add(f\"logs/train_{model_name}_{optimizer_name}_lr{learning_rate}.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4a67d2-32b7-40fd-96c9-32e42891a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initialize model and optimizer\n",
    "model, train_loader = get_model_and_dataloader(\n",
    "    model_name, dataset_name, hidden_size\n",
    ")\n",
    "\n",
    "def get_optimizer(optimizer_name, model, lr=1e-3, wd=0.1):\n",
    "    if optimizer_name == \"adamw\":\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.95)\n",
    "        )\n",
    "    elif optimizer_name == \"muon\":\n",
    "        muon_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "        ]\n",
    "        adamw_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if not (\n",
    "                p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return Muon(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            muon_params=muon_params,\n",
    "            adamw_params=adamw_params,\n",
    "        )\n",
    "    else:\n",
    "        assert 0, \"optimizer not supported\"\n",
    "optimizer = get_optimizer(\n",
    "    optimizer_name, model, lr=learning_rate, wd=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b8e7229-f303-44d3-92da-d848540c7605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((512,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((512,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((512,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8: Setup device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9761df-e3f1-44b3-8e09-c793cfd07634",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, labels\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:823\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    819\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    820\u001b[0m )\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    824\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    825\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    826\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    827\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    828\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    829\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    830\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    831\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    832\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    834\u001b[0m )\n\u001b[0;32m    836\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:549\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    537\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    538\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[0;32m    539\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m         position_embeddings,\n\u001b[0;32m    547\u001b[0m     )\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    550\u001b[0m         hidden_states,\n\u001b[0;32m    551\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    552\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    553\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    554\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    555\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    556\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    557\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    559\u001b[0m     )\n\u001b[0;32m    561\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:262\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    263\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    264\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    265\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    266\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    267\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    268\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    269\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    270\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:194\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 194\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    196\u001b[0m     query_states,\n\u001b[0;32m    197\u001b[0m     key_states,\n\u001b[0;32m    198\u001b[0m     value_states,\n\u001b[0;32m    199\u001b[0m     attention_mask,\n\u001b[0;32m    200\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    201\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    202\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39msliding_window,  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    206\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    207\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     52\u001b[0m     is_causal \u001b[38;5;241m=\u001b[39m is_causal\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 54\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m     55\u001b[0m     query,\n\u001b[0;32m     56\u001b[0m     key,\n\u001b[0;32m     57\u001b[0m     value,\n\u001b[0;32m     58\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m     59\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m     60\u001b[0m     scale\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[0;32m     61\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Cell 9: Training loop\n",
    "model.train()\n",
    "num_epochs = 2\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * num_epochs,\n",
    "    num_cycles=0.5,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        input_ids = batch\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(\n",
    "            f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']} Training loss: {loss.item()}\"\n",
    "        )\n",
    "        \n",
    "        # For Jupyter notebook display (print less frequently to avoid cluttering)\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']:.6f} Training loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ff67e-9642-47c7-99a4-b6c1700e6c26",
   "metadata": {},
   "source": [
    "# Basic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e82cd4-b042-401b-97b1-38216fd0052f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.67 GiB is allocated by PyTorch, and 20.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, labels\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:823\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    819\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    820\u001b[0m )\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    824\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    825\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    826\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    827\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    828\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    829\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    830\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    831\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    832\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    834\u001b[0m )\n\u001b[0;32m    836\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:549\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    537\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    538\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[0;32m    539\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m         position_embeddings,\n\u001b[0;32m    547\u001b[0m     )\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    550\u001b[0m         hidden_states,\n\u001b[0;32m    551\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    552\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    553\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    554\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    555\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    556\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    557\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    559\u001b[0m     )\n\u001b[0;32m    561\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:278\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 278\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    279\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    281\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:59\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 59\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\nn\\functional.py:2380\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.67 GiB is allocated by PyTorch, and 20.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Cell 9: Training loop\n",
    "model.train()\n",
    "num_epochs = 2\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * num_epochs,\n",
    "    num_cycles=0.5,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        input_ids = batch\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(\n",
    "            f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']} Training loss: {loss.item()}\"\n",
    "        )\n",
    "        \n",
    "        # For Jupyter notebook display (print less frequently to avoid cluttering)\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']:.6f} Training loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004898a5-1864-44cd-85a3-33194860f917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5a7272-48d3-4d87-b69c-370ca3d5ab09",
   "metadata": {},
   "source": [
    "# To run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e5dd30-9557-4881-a15c-825baa09f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized data from openwebtext-100k.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 1 \n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from loguru import logger\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    Qwen2Config,\n",
    "    Qwen2ForCausalLM,\n",
    "    Qwen2Tokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "\n",
    "# cell 2\n",
    "class MoonDataset(Dataset):\n",
    "    def __init__(self, dataset_name, dataset, tokenizer, max_length=256):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = dataset[\"train\"][\"text\"]\n",
    "        self.max_length = max_length\n",
    "        self.tokens = []\n",
    "        self._tokenize_texts()\n",
    "\n",
    "    def _tokenize_texts(self):\n",
    "        cache_path = f\"{self.dataset_name}.bin\"\n",
    "        if os.path.exists(cache_path):\n",
    "            self.tokens = torch.load(cache_path)\n",
    "            print(f\"Loaded tokenized data from {cache_path}\")\n",
    "        else:\n",
    "            print(\"Tokenizing texts...\")\n",
    "            all_tokens = []\n",
    "            for text in tqdm(self.texts, desc=\"Tokenizing texts\"):\n",
    "                # Add proper tokenization with padding/truncation\n",
    "                encoded = self.tokenizer.encode(text, add_special_tokens=True, \n",
    "                                               max_length=self.max_length, \n",
    "                                               truncation=True)\n",
    "                all_tokens.extend(encoded)\n",
    "            self.tokens = all_tokens\n",
    "            torch.save(self.tokens, cache_path)\n",
    "            print(f\"Saved tokenized data to {cache_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.tokens) // self.max_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.max_length\n",
    "        end_idx = min(start_idx + self.max_length, len(self.tokens))\n",
    "        \n",
    "        # Make sure we have exactly max_length tokens\n",
    "        token_slice = self.tokens[start_idx:end_idx]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(token_slice) < self.max_length:\n",
    "            token_slice = token_slice + [self.tokenizer.pad_token_id] * (self.max_length - len(token_slice))\n",
    "            \n",
    "        data = torch.tensor(token_slice, dtype=torch.long)\n",
    "        return data\n",
    "\n",
    "# cell 3\n",
    "# This code snippet is a modified version adapted from the following GitHub repository:\n",
    "# https://github.com/KellerJordan/Muon/blob/master/muon.py\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = (\n",
    "            b * A + c * A @ A\n",
    "        )  # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "# cell 4\n",
    "class Muon_default(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Cell 5\n",
    "def get_model_and_dataloader(model_name, dataset_name, hidden_size):\n",
    "    name2path = {\n",
    "        \"openwebtext-100k\": \"Elriggs/openwebtext-100k\",\n",
    "    }\n",
    "    train_dataset = load_dataset(name2path[dataset_name], trust_remote_code=True)\n",
    "    if model_name == \"qwen\":\n",
    "        tokenizer = Qwen2Tokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-0.5B\", trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        assert 0, f\"model {model_name} not supported\"\n",
    "    train_dataset = MoonDataset(dataset_name, train_dataset, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    if model_name == \"qwen\":\n",
    "        config = Qwen2Config(\n",
    "            attention_dropout=0.0,\n",
    "            bos_token_id=151643,\n",
    "            eos_token_id=151643,\n",
    "            hidden_act=\"silu\",\n",
    "            hidden_size=hidden_size,\n",
    "            initializer_range=0.02,\n",
    "            intermediate_size=4864, # Reduce from 4864,\n",
    "            max_position_embeddings=513,\n",
    "            max_window_layers=12,\n",
    "            model_type=\"qwen2\",\n",
    "            num_hidden_layers=12,  # Reduce from 12\n",
    "            num_attention_heads=16,  # Reduce from 16\n",
    "            num_key_value_heads=16,\n",
    "            rms_norm_eps=1e-06,\n",
    "            rope_theta=1000000.0,\n",
    "            sliding_window=1024,\n",
    "            tie_word_embeddings=True,\n",
    "            torch_dtype=\"bfloat16\",\n",
    "            use_cache=True,\n",
    "            use_mrope=False,\n",
    "            use_sliding_window=False,\n",
    "            vocab_size=151936,\n",
    "        )\n",
    "        model = Qwen2ForCausalLM(config)\n",
    "    else:\n",
    "        assert 0, f\"model {model_name} not supported\"\n",
    "    return model, train_loader\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name, model, lr=1e-3, wd=0.1):\n",
    "    if optimizer_name == \"adamw\":\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.95)\n",
    "        )\n",
    "    elif optimizer_name == \"muon\":\n",
    "        muon_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "        ]\n",
    "        adamw_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if not (\n",
    "                p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return Muon_default(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            muon_params=muon_params,\n",
    "            adamw_params=adamw_params,\n",
    "        )\n",
    "    else:\n",
    "        assert 0, \"optimizer not supported\"\n",
    "\n",
    "# Cell 6: Configure training parameters\n",
    "# Set your parameters here\n",
    "model_name = \"qwen\"  # Options: \"qwen\"\n",
    "optimizer_name = \"muon\"  # Options: \"adamw\", \"muon\"\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.1\n",
    "dataset_name = \"openwebtext-100k\"\n",
    "hidden_size = 1024\n",
    "\n",
    "# Setup logging\n",
    "logger.add(f\"logs/train_{model_name}_{optimizer_name}_lr{learning_rate}.log\")\n",
    "\n",
    "\n",
    "# Cell 7: Initialize model and optimizer\n",
    "model, train_loader = get_model_and_dataloader(\n",
    "    model_name, dataset_name, hidden_size\n",
    ")\n",
    "optimizer = get_optimizer(\n",
    "    optimizer_name, model, lr=learning_rate, wd=weight_decay\n",
    ")\n",
    "\n",
    "\n",
    "# Cell 8: Setup device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cell 9: Training loop (Fixed version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3326d5fb-3818-4aa8-bb80-d98834d943c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_8752\\3092051002.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For mixed precision training\n",
      "W0413 22:07:41.243000 8752 site-packages\\torch\\_inductor\\utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "C:\\Users\\Professional\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1948: UserWarning: NVIDIA GeForce GTX 1060 6GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0 LR: 0.000010 Training loss: 12.1447\n",
      "Epoch: 0 Step: 50 LR: 0.000510 Training loss: 7.7767\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     41\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 196\u001b[0m, in \u001b[0;36mMuon_default.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     g \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m--> 196\u001b[0m u \u001b[38;5;241m=\u001b[39m zeropower_via_newtonschulz5(g, steps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# scale update\u001b[39;00m\n\u001b[0;32m    199\u001b[0m adjusted_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust_lr_for_muon(lr, p\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuro_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    571\u001b[0m )\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    579\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[1], line 99\u001b[0m, in \u001b[0;36mzeropower_via_newtonschulz5\u001b[1;34m(G, steps)\u001b[0m\n\u001b[0;32m     95\u001b[0m     A \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     96\u001b[0m     B \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     97\u001b[0m         b \u001b[38;5;241m*\u001b[39m A \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m*\u001b[39m A \u001b[38;5;241m@\u001b[39m A\n\u001b[0;32m     98\u001b[0m     )  \u001b[38;5;66;03m# adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     X \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m B \u001b[38;5;241m@\u001b[39m X\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m G\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    102\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 9: Training loop (Fixed version)\n",
    "model.train()\n",
    "num_epochs = 2\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * num_epochs,\n",
    "    num_cycles=0.5,\n",
    ")\n",
    "\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "lr_params = []\n",
    "step_params = []\n",
    "loss_params = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Make sure batch has the right shape [batch_size, sequence_length]\n",
    "        if len(batch.shape) == 1:\n",
    "            batch = batch.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "        # For causal language modeling, inputs are the tokens and labels are the same tokens\n",
    "        # The model will automatically shift the labels by one position\n",
    "        inputs = batch\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Log to file\n",
    "        # logger.info(\n",
    "        #     f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']} Training loss: {loss.item()}\"\n",
    "        # )\n",
    "        \n",
    "        # For Jupyter notebook display (print less frequently to avoid cluttering)\n",
    "        if step % 50 == 0:\n",
    "            lr_params.append(optimizer.param_groups[0]['lr'])\n",
    "            step_params.append(step)\n",
    "            loss_params.append(loss.item())\n",
    "            print(f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']:.6f} Training loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfee826-5813-4bbb-abe7-cc7515581c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8e0a9-c72d-4896-b57c-74e3b62dfd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b71e94-ca5b-4ffa-85bb-ed55f40efefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ee203-79b9-4a0a-a5ca-715834885741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acaeb274-f665-4a13-b7f6-b410ba718cc5",
   "metadata": {},
   "source": [
    "# To ask from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c4c64-4ba4-44c3-876c-905403cdcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1 \n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from loguru import logger\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    Qwen2Config,\n",
    "    Qwen2ForCausalLM,\n",
    "    Qwen2Tokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "\n",
    "# cell 2\n",
    "class MoonDataset(Dataset):\n",
    "    def __init__(self, dataset_name, dataset, tokenizer, max_length=256):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = dataset[\"train\"][\"text\"]\n",
    "        self.max_length = max_length\n",
    "        self.tokens = []\n",
    "        self._tokenize_texts()\n",
    "\n",
    "    def _tokenize_texts(self):\n",
    "        cache_path = f\"{self.dataset_name}.bin\"\n",
    "        if os.path.exists(cache_path):\n",
    "            self.tokens = torch.load(cache_path)\n",
    "            print(f\"Loaded tokenized data from {cache_path}\")\n",
    "        else:\n",
    "            print(\"Tokenizing texts...\")\n",
    "            all_tokens = []\n",
    "            for text in tqdm(self.texts, desc=\"Tokenizing texts\"):\n",
    "                # Add proper tokenization with padding/truncation\n",
    "                encoded = self.tokenizer.encode(text, add_special_tokens=True, \n",
    "                                               max_length=self.max_length, \n",
    "                                               truncation=True)\n",
    "                all_tokens.extend(encoded)\n",
    "            self.tokens = all_tokens\n",
    "            torch.save(self.tokens, cache_path)\n",
    "            print(f\"Saved tokenized data to {cache_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.tokens) // self.max_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.max_length\n",
    "        end_idx = min(start_idx + self.max_length, len(self.tokens))\n",
    "        \n",
    "        # Make sure we have exactly max_length tokens\n",
    "        token_slice = self.tokens[start_idx:end_idx]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(token_slice) < self.max_length:\n",
    "            token_slice = token_slice + [self.tokenizer.pad_token_id] * (self.max_length - len(token_slice))\n",
    "            \n",
    "        data = torch.tensor(token_slice, dtype=torch.long)\n",
    "        return data\n",
    "\n",
    "# cell 3\n",
    "# This code snippet is a modified version adapted from the following GitHub repository:\n",
    "# https://github.com/KellerJordan/Muon/blob/master/muon.py\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = (\n",
    "            b * A + c * A @ A\n",
    "        )  # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "# cell 4\n",
    "class Muon_default(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        wd=0.1,\n",
    "        muon_params=None,\n",
    "        momentum=0.95,\n",
    "        nesterov=True,\n",
    "        ns_steps=5,\n",
    "        adamw_params=None,\n",
    "        adamw_betas=(0.9, 0.95),\n",
    "        adamw_eps=1e-8,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov,\n",
    "            ns_steps=ns_steps,\n",
    "            adamw_betas=adamw_betas,\n",
    "            adamw_eps=adamw_eps,\n",
    "        )\n",
    "\n",
    "        params = list(muon_params)\n",
    "        adamw_params = list(adamw_params) if adamw_params is not None else []\n",
    "        params.extend(adamw_params)\n",
    "        super().__init__(params, defaults)\n",
    "        # Sort parameters into those for which we will use Muon, and those for which we will not\n",
    "        for p in muon_params:\n",
    "            # Use Muon for every parameter in muon_params which is >= 2D and doesn't look like an embedding or head layer\n",
    "            assert p.ndim == 2, p.ndim\n",
    "            self.state[p][\"use_muon\"] = True\n",
    "        for p in adamw_params:\n",
    "            # Do not use Muon for parameters in adamw_params\n",
    "            self.state[p][\"use_muon\"] = False\n",
    "\n",
    "    def adjust_lr_for_muon(self, lr, param_shape):\n",
    "        A, B = param_shape[:2]\n",
    "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
    "        # as describted in the paper\n",
    "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
    "        adjusted_lr = lr * adjusted_ratio\n",
    "        return adjusted_lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            ############################\n",
    "            #           Muon           #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if self.state[p][\"use_muon\"]]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"wd\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            for p in params:\n",
    "                # sanity check\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                if g.ndim > 2:\n",
    "                    g = g.view(g.size(0), -1)\n",
    "                assert g is not None\n",
    "\n",
    "                # calc update\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                if group[\"nesterov\"]:\n",
    "                    g = g.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    g = buf\n",
    "                u = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # scale update\n",
    "                adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
    "\n",
    "                # apply weight decay\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                # apply update\n",
    "                p.data.add_(u, alpha=-adjusted_lr)\n",
    "\n",
    "            ############################\n",
    "            #       AdamW backup       #\n",
    "            ############################\n",
    "\n",
    "            params = [p for p in group[\"params\"] if not self.state[p][\"use_muon\"]]\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group[\"adamw_betas\"]\n",
    "            eps = group[\"adamw_eps\"]\n",
    "            weight_decay = group[\"wd\"]\n",
    "\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"moment1\"] = torch.zeros_like(g)\n",
    "                    state[\"moment2\"] = torch.zeros_like(g)\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "                buf1 = state[\"moment1\"]\n",
    "                buf2 = state[\"moment2\"]\n",
    "                buf1.lerp_(g, 1 - beta1)\n",
    "                buf2.lerp_(g.square(), 1 - beta2)\n",
    "\n",
    "                g = buf1 / (eps + buf2.sqrt())\n",
    "\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "                scale = bias_correction1 / bias_correction2**0.5\n",
    "                p.data.mul_(1 - lr * weight_decay)\n",
    "                p.data.add_(g, alpha=-lr / scale)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Cell 5\n",
    "def get_model_and_dataloader(model_name, dataset_name, hidden_size):\n",
    "    name2path = {\n",
    "        \"openwebtext-100k\": \"Elriggs/openwebtext-100k\",\n",
    "    }\n",
    "    train_dataset = load_dataset(name2path[dataset_name], trust_remote_code=True)\n",
    "    if model_name == \"qwen\":\n",
    "        tokenizer = Qwen2Tokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-0.5B\", trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        assert 0, f\"model {model_name} not supported\"\n",
    "    train_dataset = MoonDataset(dataset_name, train_dataset, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    if model_name == \"qwen\":\n",
    "        config = Qwen2Config(\n",
    "            attention_dropout=0.0,\n",
    "            bos_token_id=151643,\n",
    "            eos_token_id=151643,\n",
    "            hidden_act=\"silu\",\n",
    "            hidden_size=hidden_size,\n",
    "            initializer_range=0.02,\n",
    "            intermediate_size=4864, # Reduce from 4864,\n",
    "            max_position_embeddings=513,\n",
    "            max_window_layers=12,\n",
    "            model_type=\"qwen2\",\n",
    "            num_hidden_layers=12,  # Reduce from 12\n",
    "            num_attention_heads=16,  # Reduce from 16\n",
    "            num_key_value_heads=16,\n",
    "            rms_norm_eps=1e-06,\n",
    "            rope_theta=1000000.0,\n",
    "            sliding_window=1024,\n",
    "            tie_word_embeddings=True,\n",
    "            torch_dtype=\"bfloat16\",\n",
    "            use_cache=True,\n",
    "            use_mrope=False,\n",
    "            use_sliding_window=False,\n",
    "            vocab_size=151936,\n",
    "        )\n",
    "        model = Qwen2ForCausalLM(config)\n",
    "    else:\n",
    "        assert 0, f\"model {model_name} not supported\"\n",
    "    return model, train_loader\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name, model, lr=1e-3, wd=0.1):\n",
    "    if optimizer_name == \"adamw\":\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.95)\n",
    "        )\n",
    "    elif optimizer_name == \"muon\":\n",
    "        muon_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "        ]\n",
    "        adamw_params = [\n",
    "            p\n",
    "            for name, p in model.named_parameters()\n",
    "            if not (\n",
    "                p.ndim >= 2 and \"embed_tokens\" not in name and \"lm_head\" not in name\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return Muon_default(\n",
    "            lr=lr,\n",
    "            wd=wd,\n",
    "            muon_params=muon_params,\n",
    "            adamw_params=adamw_params,\n",
    "        )\n",
    "    else:\n",
    "        assert 0, \"optimizer not supported\"\n",
    "\n",
    "# Cell 6: Configure training parameters\n",
    "# Set your parameters here\n",
    "model_name = \"qwen\"  # Options: \"qwen\"\n",
    "optimizer_name = \"muon\"  # Options: \"adamw\", \"muon\"\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.1\n",
    "dataset_name = \"openwebtext-100k\"\n",
    "hidden_size = 1024\n",
    "\n",
    "# Setup logging\n",
    "logger.add(f\"logs/train_{model_name}_{optimizer_name}_lr{learning_rate}.log\")\n",
    "\n",
    "\n",
    "# Cell 7: Initialize model and optimizer\n",
    "model, train_loader = get_model_and_dataloader(\n",
    "    model_name, dataset_name, hidden_size\n",
    ")\n",
    "optimizer = get_optimizer(\n",
    "    optimizer_name, model, lr=learning_rate, wd=weight_decay\n",
    ")\n",
    "\n",
    "\n",
    "# Cell 8: Setup device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cell 9: Training loop (Fixed version)\n",
    "model.train()\n",
    "num_epochs = 2\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * num_epochs,\n",
    "    num_cycles=0.5,\n",
    ")\n",
    "\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Make sure batch has the right shape [batch_size, sequence_length]\n",
    "        if len(batch.shape) == 1:\n",
    "            batch = batch.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "        # For causal language modeling, inputs are the tokens and labels are the same tokens\n",
    "        # The model will automatically shift the labels by one position\n",
    "        inputs = batch\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(\n",
    "            f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']} Training loss: {loss.item()}\"\n",
    "        )\n",
    "        \n",
    "        # For Jupyter notebook display (print less frequently to avoid cluttering)\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Epoch: {epoch} Step: {step} LR: {optimizer.param_groups[0]['lr']:.6f} Training loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c33f9-acd7-43b2-bb53-1d125032891d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfa74a-f1a8-4a99-a172-6609edd8afb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f20be4-0c38-4217-aebc-9effcbff110f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fce66-e059-4511-bf37-ab095af463bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db1c2a-f040-45ea-aec5-fb2eb1b72050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
